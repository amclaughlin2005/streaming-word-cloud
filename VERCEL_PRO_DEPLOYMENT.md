# üöÄ Vercel Pro Deployment Guide
**Moving Beyond Serverless Limits for ML-Heavy Applications**

## Current Status ‚úÖ
Your application now uses a **hybrid architecture** that works with Vercel Pro:
- ‚úÖ **Lightweight Node.js APIs** handle data upload/storage (under limits)
- ‚úÖ **Simple analysis functions** provide basic word frequency and sentiment analysis
- ‚úÖ **External ML service integration** for advanced word cloud generation
- ‚úÖ **Graceful fallbacks** ensure the app always works

## Deployment Options with Vercel Pro

### Option 1: **Immediate Deployment** (Recommended for Quick Start) üéØ

**What works right now:**
```bash
npm run vercel-build  # Should deploy successfully
```

**Features available:**
- ‚úÖ Data upload to Vercel Blob storage
- ‚úÖ Basic sentiment analysis (positive/negative/neutral)
- ‚úÖ Question type categorization (what/how/why/etc.)
- ‚úÖ Simple word frequency analysis
- ‚ö†Ô∏è Advanced word clouds require external service (see below)

### Option 2: **External ML Service** (Production Recommended) üè¢

Deploy the heavy ML processing on a separate service:

#### **A. Railway.app Deployment** (Easy & Cost-Effective)
```bash
# 1. Create railway.app account
# 2. Install Railway CLI
npm install -g @railway/cli

# 3. Login and deploy ML service
railway login
railway init
railway up
```

**Dockerfile for Railway:**
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8080
CMD ["python", "api/analysis.py"]
```

#### **B. DigitalOcean App Platform**
```bash
# Deploy via GitHub integration
# Set app to use Dockerfile
# Environment: ML_SERVICE_URL=https://your-ml-service.ondigitalocean.app
```

#### **C. Google Cloud Run** (Serverless)
```bash
gcloud run deploy wordcloud-ml \
  --source . \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

**Then set environment variable in Vercel:**
```bash
ML_SERVICE_URL=https://your-service-url.run.app
```

### Option 3: **Vercel Pro Functions with Optimizations** üîß

If you want to keep everything on Vercel, try these Pro-level optimizations:

#### **A. Use Vercel's New Docker Support (Beta)**
```json
// vercel.json
{
  "functions": {
    "api/analysis.py": {
      "runtime": "container",
      "memory": 3008,
      "maxDuration": 60
    }
  }
}
```

#### **B. Edge Functions for Simple Analysis**
```javascript
// api/edge/simple-analysis.js
export default function handler(req) {
  // Ultra-lightweight processing
  // Runs at the edge with minimal latency
}
```

#### **C. Pre-computed Analysis with Cron**
```json
// vercel.json
{
  "crons": [
    {
      "path": "/api/cron/precompute",
      "schedule": "0 */6 * * *"  // Every 6 hours
    }
  ]
}
```

### Option 4: **Static Pre-Generation** ‚ö°

Generate word clouds in advance and serve them statically:

```javascript
// api/cron/precompute.js
module.exports = async function handler(req, res) {
  // 1. Fetch latest data from Vercel Blob
  // 2. Generate word clouds for common views
  // 3. Save as static images in public/
  // 4. Update manifest.json with available analyses
}
```

## Configuration Steps üõ†Ô∏è

### 1. **Environment Variables** (Vercel Dashboard)
```bash
# Required
BLOB_READ_WRITE_TOKEN=vercel_blob_rw_...

# Optional (for external ML service)
ML_SERVICE_URL=https://your-ml-service.com
ML_SERVICE_API_KEY=your-api-key

# Pro Features
VERCEL_ANALYTICS_ID=your-analytics-id
```

### 2. **Vercel Pro Limits** (Current Status)
- ‚úÖ **Function Memory**: Up to 3008MB (vs 1024MB free)
- ‚úÖ **Function Duration**: Up to 300s (vs 10s free) 
- ‚úÖ **Function Size**: Still 250MB limit, but Docker containers available
- ‚úÖ **Concurrent Executions**: 1000 (vs 100 free)
- ‚úÖ **Edge Functions**: 4MB limit, global distribution

### 3. **Monitoring & Optimization**
```javascript
// Add to your functions for Pro monitoring
console.log('Memory usage:', process.memoryUsage());
console.log('Execution time:', Date.now() - startTime);
```

## Recommended Setup for Production üèÜ

```bash
# 1. Deploy lightweight version to Vercel (works immediately)
vercel deploy

# 2. Deploy ML service to Railway/DigitalOcean 
railway up  # or your preferred platform

# 3. Configure ML_SERVICE_URL in Vercel environment
vercel env add ML_SERVICE_URL

# 4. Enable Pro monitoring
vercel analytics enable
```

## Cost Estimation üí∞

**Vercel Pro:** $20/month
- Unlimited bandwidth
- Advanced functions
- Team collaboration

**External ML Service:**
- **Railway:** ~$5-15/month (based on usage)
- **DigitalOcean:** ~$10-25/month (fixed pricing)
- **Google Cloud Run:** ~$0-10/month (pay-per-request)

**Total:** ~$25-45/month for production-ready ML application

## Testing Your Deployment üß™

```bash
# Test basic functionality
curl https://your-app.vercel.app/api/sentiment-data

# Test external ML integration
curl https://your-app.vercel.app/api/wordcloud-data

# Test data upload
curl -X POST https://your-app.vercel.app/api/upload-data \
  -H "Content-Type: application/json" \
  -d '{"csvData": [...]}'
```

## Troubleshooting üîß

### **Still getting 250MB errors?**
1. Check you're using `analysis-proxy.js` not `analysis.py`
2. Verify no heavy dependencies in Node.js functions
3. Use external service for heavy ML processing

### **External service not responding?**
1. Check ML_SERVICE_URL environment variable
2. Verify external service is running
3. Check CORS headers on external service
4. App falls back to simple analysis automatically

### **Performance issues?**
1. Enable Vercel Analytics to identify bottlenecks
2. Use Edge Functions for simple operations
3. Implement caching for repeated analyses
4. Limit data processing to 1000 records max

## Next Steps üéØ

1. **Deploy immediately** with current lightweight setup
2. **Add external ML service** for advanced features  
3. **Monitor usage** and optimize based on real traffic
4. **Scale gradually** as your user base grows

Your app is now **production-ready** with Vercel Pro! üéâ 